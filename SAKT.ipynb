{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99f5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import psutil\n",
    "import joblib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39db5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMPLES = 320000\n",
    "\n",
    "MAX_SEQ = 100\n",
    "MIN_SAMPLES = 5\n",
    "EMBED_DIM = 128\n",
    "DROPOUT_RATE = 0.2\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_LEARNING_RATE = 2e-3\n",
    "# EPOCHS = 30\n",
    "EPOCHS = 10\n",
    "# TRAIN_BATCH_SIZE = 2048\n",
    "TRAIN_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca627e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80833b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number skills 13523\n",
      "TRAIN_SAMPLES 314924\n",
      "train_group \n",
      " user_id\n",
      "115     ([5692, 5716, 128, 7860, 7922, 156, 51, 50, 78...\n",
      "124     ([7900, 7876, 175, 1278, 2065, 2063, 2064, 336...\n",
      "2746    ([5273, 758, 5976, 236, 404, 382, 405, 873, 53...\n",
      "5382    ([5000, 3944, 217, 5844, 5965, 4990, 5235, 605...\n",
      "8623    ([3915, 4750, 6456, 3968, 6104, 5738, 6435, 54...\n",
      "dtype: object\n",
      "valid_group \n",
      " user_id\n",
      "1720820513    ([3849, 1320, 5285, 8918, 3644, 6111, 8397, 94...\n",
      "1720823127    ([7900, 7876, 175, 1278, 2064, 2065, 2063, 336...\n",
      "1720823509    ([128, 7860, 7922, 156, 51, 50, 7896, 7863, 15...\n",
      "1720827508    ([7900, 7876, 175, 1278, 2065, 2063, 2064, 336...\n",
      "1720827841    ([7900, 7876, 175, 1278, 2065, 2063, 2064, 336...\n",
      "dtype: object\n",
      "314924 78732\n",
      "CPU times: user 55.6 s, sys: 8.47 s, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dtypes = {'timestamp': 'int64', 'user_id': 'int32' ,'content_id': 'int16','content_type_id': 'int8','answered_correctly':'int8'}\n",
    "# train_df = pd.read_feather('../input/riiid-cross-validation-dataset/train.feather')[[\n",
    "#     'timestamp', 'user_id', 'content_id', 'content_type_id', 'answered_correctly'\n",
    "# ]]\n",
    "train_df = pd.read_csv('./input/riiid-test-answer-prediction/train.csv')[['timestamp', 'user_id', 'content_id', 'content_type_id', 'answered_correctly']]\n",
    "for col, dtype in dtypes.items():\n",
    "    train_df[col] = train_df[col].astype(dtype)\n",
    "    \n",
    "    \n",
    "#train_df have only rows with False in content_type_id (0 if the event was a question being posed to the user)\n",
    "train_df = train_df[train_df.content_type_id == False]  \n",
    "\n",
    "train_df = train_df.sort_values(['timestamp'], ascending=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "skills = train_df[\"content_id\"].unique()\n",
    "# joblib.dump(skills, \"skills.pkl.zip\")\n",
    "n_skill = len(skills)  # (unique content IDs)\n",
    "print(\"number skills\", n_skill)\n",
    "\n",
    "\n",
    "group = train_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n",
    "            r['content_id'].values,\n",
    "            r['answered_correctly'].values))\n",
    "# joblib.dump(group, \"group.pkl.zip\")  # Save models?\n",
    "del train_df\n",
    "gc.collect()\n",
    "# group\n",
    "\n",
    "# The training data is sorted by timestamp and split into two sets using an 80/20 split\n",
    "TRAIN_SAMPLES = int(len(group.index)*0.8)\n",
    "print('TRAIN_SAMPLES',TRAIN_SAMPLES)\n",
    "\n",
    "\n",
    "# The method then creates a dictionary of samples, where each key is a user ID and the corresponding value is a tuple containing the user's content IDs and answered correctly values.\n",
    "train_indexes = list(group.index)[:TRAIN_SAMPLES]\n",
    "valid_indexes = list(group.index)[TRAIN_SAMPLES:]\n",
    "train_group = group[group.index.isin(train_indexes)]\n",
    "valid_group = group[group.index.isin(valid_indexes)]\n",
    "print('train_group \\n', train_group[:5] )\n",
    "print('valid_group \\n', valid_group[:5] )\n",
    "\n",
    "del group, train_indexes, valid_indexes\n",
    "print(len(train_group), len(valid_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6aa38c",
   "metadata": {},
   "source": [
    "### The BERTDataset class returns three values for each sample in the dataset:\n",
    "\n",
    "- `x`: the input data for the BERT model, which consists of the content IDs for each sample shifted by one and the answered correctly values added to the content IDs\n",
    "- `target_id`: the target IDs for each sample, which consist of the content IDs shifted by one\n",
    "- `label`: the labels for each sample, which consist of the answered correctly values shifted by one\n",
    "\n",
    "These values are used as input to the BERT model and are used to calculate the model's performance during training and evaluation. The `x` and `target_id` arrays are used as input to the BERT model, while the `label` array is used to calculate the model's loss and accuracy.\n",
    "\n",
    "The __init__ method iterates over the users in the `group` object and retrieves the questions and answers for each user. If a user has answered fewer than `min_samples` questions, their questions and answers are not included in the `samples` dictionary. If a user has answered more than `max_seq` questions, their questions and answers are split into multiple sequences of length `max_seq` and each sequence is added to the `samples` dictionary using a unique key that includes the user's ID and the sequence number. For example, if the user's ID is `123` and they have answered 150 questions, their questions and answers will be split into two sequences with lengths 128 and 22, and the keys `123_0` and `123_1` will be added to the samples dictionary with the values of the first and second sequence, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5f96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, group, n_skill, min_samples=1, max_seq=128):\n",
    "        super(BERTDataset, self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        self.n_skill = n_skill\n",
    "        self.samples = {}\n",
    "        \n",
    "        self.user_ids = []\n",
    "        for user_id in group.index:\n",
    "            q, qa = group[user_id]  # q:content_id(questions); qa:answered_correctly(user's question answer)\n",
    "            if len(q) < min_samples:  # If a user has answered fewer than min_samples questions, their questions and answers are not included in the 'samples' dictionary\n",
    "                continue \n",
    "            \n",
    "            # Main Contribution\n",
    "            if len(q) > self.max_seq:\n",
    "                total_questions = len(q)\n",
    "                initial = total_questions % self.max_seq\n",
    "                if initial >= min_samples:\n",
    "                    self.user_ids.append(f\"{user_id}_0\")\n",
    "                    self.samples[f\"{user_id}_0\"] = (q[:initial], qa[:initial])\n",
    "                for seq in range(total_questions // self.max_seq):\n",
    "                    self.user_ids.append(f\"{user_id}_{seq+1}\")\n",
    "                    start = initial + seq * self.max_seq\n",
    "                    end = start + self.max_seq\n",
    "                    self.samples[f\"{user_id}_{seq+1}\"] = (q[start:end], qa[start:end])\n",
    "            else:\n",
    "                user_id = str(user_id)\n",
    "                self.user_ids.append(user_id)\n",
    "                self.samples[user_id] = (q, qa)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user_id = self.user_ids[index]\n",
    "        q_, qa_ = self.samples[user_id]\n",
    "        seq_len = len(q_)\n",
    "\n",
    "        q = np.zeros(self.max_seq, dtype=int)\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        if seq_len == self.max_seq:\n",
    "            q[:] = q_\n",
    "            qa[:] = qa_\n",
    "        else:\n",
    "            q[-seq_len:] = q_\n",
    "            qa[-seq_len:] = qa_\n",
    "        \n",
    "        # 'x' also has a length of max_seq-1\n",
    "        target_id = q[1:]  \n",
    "        label = qa[1:]\n",
    "        \n",
    "        x = np.zeros(self.max_seq-1, dtype=int)\n",
    "        x = q[:-1].copy()\n",
    "        x += (qa[:-1] == 1) * self.n_skill  # the model needs to be able to distinguish between the question IDs and the correct answers in order to make predictions.\n",
    "          \n",
    "        return x, target_id, label\n",
    "    \n",
    "    \n",
    "train_dataset = BERTDataset(train_group, n_skill, min_samples=MIN_SAMPLES, max_seq=MAX_SEQ)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "valid_dataset = BERTDataset(valid_group, n_skill, max_seq=MAX_SEQ)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2eb87",
   "metadata": {},
   "source": [
    "## Define model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba54a3",
   "metadata": {},
   "source": [
    "\n",
    "- the `d_model` parameter specifies the size of the hidden states used by the model. This is also known as the \"model size\" or the \"embedding size\" of the model.\n",
    "- The `d_model` parameter is used as a scaling factor when computing the dot product between the query and key vectors in the Attention mechanism. It is also used to specify the size of the input and output vectors for the linear layers in the MultiHeadedAttention class, as well as the size of the input and output vectors for the LayerNorm and SublayerConnection classes. In general, a larger d_model value will result in a more expressive BERT model, but will also increase the computational complexity and memory usage of the model.\n",
    "\n",
    "\n",
    "\n",
    "### Sublayer Connection:\n",
    "- ### residual connection:\n",
    "    - A residual connection is a type of connection in a neural network that allows information to bypass one or more layers of the network. This allows the network to learn to perform tasks more efficiently by allowing the information to flow more directly from the input to the output layers. Residual connections can help improve the performance of the network, particularly on tasks that require the network to process long sequences of data. They are often used in deep learning networks, where they can help prevent the vanishing gradient problem, allowing the network to learn more effectively.\n",
    "    \n",
    "- ### LayerNorm:\n",
    "    - the `eps` parameter specifies a small value used to stabilize the division operation in the layer normalization computation. This is necessary because division by zero is undefined, and division by a very small value can lead to numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca561c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, state_size=200):\n",
    "        super(FFN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.lr1 = nn.Linear(state_size, state_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(state_size, state_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lr2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def future_mask(seq_length):\n",
    "    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "    return torch.from_numpy(future_mask)\n",
    "\n",
    "\n",
    "class SAKTModel(nn.Module):\n",
    "    def __init__(self, n_skill, max_seq=128, embed_dim=128, dropout_rate=0.2):\n",
    "        super(SAKTModel, self).__init__()\n",
    "        self.n_skill = n_skill\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "# self.embedding: an nn.Embedding layer that maps the input sequence of student responses to a sequence of vectors of dimension embed_dim\n",
    "# self.pos_embedding: an nn.Embedding layer that maps the positions of each token in the student response to a vector of dimension embed_dim\n",
    "# self.e_embedding: an nn.Embedding layer that maps the input sequence of question IDs to a sequence of vectors of dimension embed_dim\n",
    "# self.multi_att: an nn.MultiheadAttention layer that computes attention weights for each student response\n",
    "        self.embedding = nn.Embedding(2*n_skill+1, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq-1, embed_dim)\n",
    "        self.e_embedding = nn.Embedding(n_skill+1, embed_dim)\n",
    "\n",
    "        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=dropout_rate)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_normal = nn.LayerNorm(embed_dim) \n",
    "\n",
    "        self.ffn = FFN(embed_dim)\n",
    "        self.pred = nn.Linear(embed_dim, 1)\n",
    "    \n",
    "    def forward(self, x, question_ids):\n",
    "        device = x.device        \n",
    "        x = self.embedding(x)\n",
    "        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n",
    "\n",
    "        pos_x = self.pos_embedding(pos_id)\n",
    "        x = x + pos_x\n",
    "\n",
    "        e = self.e_embedding(question_ids)\n",
    "\n",
    "        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n",
    "        e = e.permute(1, 0, 2)\n",
    "        att_mask = future_mask(x.size(0)).to(device)\n",
    "        att_output, att_weight = self.multi_att(e, x, x, attn_mask=att_mask)\n",
    "#         att_output = self.multi_att(e, x, x, attn_mask=att_mask)\n",
    "        att_output = self.layer_normal(att_output + e)\n",
    "        att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n",
    "\n",
    "        x = self.ffn(att_output)\n",
    "        x = self.layer_normal(x + att_output)\n",
    "        x = self.pred(x)\n",
    "\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b526c13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13523"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6ef645d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAKTModel(\n",
       "  (embedding): Embedding(27047, 128)\n",
       "  (pos_embedding): Embedding(99, 128)\n",
       "  (e_embedding): Embedding(13524, 128)\n",
       "  (multi_att): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (layer_normal): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (ffn): FFN(\n",
       "    (lr1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (lr2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (pred): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SAKTModel(n_skill, max_seq=MAX_SEQ, embed_dim=EMBED_DIM, dropout_rate=DROPOUT_RATE)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8690d",
   "metadata": {},
   "source": [
    "## Define Train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28bedd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, dataloader, optimizer, scheduler, criterion, device=\"cpu\"):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "\n",
    "    for item in dataloader:\n",
    "#         x = item[0]\n",
    "#         print('x',x, 'x.size()',x.size())\n",
    "        x = item[0].to(device).long()\n",
    "        segment_info = item[1].to(device).long()\n",
    "        label = item[2].to(device).float()\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, segment_info)\n",
    "#         print('output',output, 'output.size()',output.size())\n",
    "        \n",
    "        \n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        \n",
    "        target_mask = (segment_info != 0)  # Create a mask indicating which values in segment_info are not 0\n",
    "        last_nonzero_idx = target_mask.sum(dim=1) - 1  # Find the last non-zero value for each batch\n",
    "        \n",
    "        output = output[range(last_nonzero_idx.size()[0]), last_nonzero_idx]  # Index into the output tensor using last_nonzero_idx\n",
    "        label = label[range(last_nonzero_idx.size()[0]), last_nonzero_idx]  # Index into the label tensor using last_nonzero_idx\n",
    "\n",
    "#         print('output = output[range(64), last_nonzero_idx]', output,'output.size()',output.size())\n",
    "    \n",
    "        pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         output = torch.masked_select(output, target_mask)\n",
    "#         label = torch.masked_select(label, target_mask)\n",
    "#         pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "        \n",
    "#         print('output = torch.masked_select(output, target_mask)', output,'output.size()',output.size())\n",
    "        \n",
    "#         print('label = torch.masked_select(label, target_mask)', label,'label.size()',label.size())\n",
    "#         print('pred = (torch.sigmoid(output) >= 0.5).long()', pred, 'pred.size()',pred.size())\n",
    "        \n",
    "        num_corrects += (pred == label).sum().item()\n",
    "        num_total += len(label)\n",
    "\n",
    "        labels.extend(label.view(-1).data.cpu().numpy())\n",
    "        outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "    loss = np.mean(train_loss)\n",
    "\n",
    "    return loss, acc, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72be72",
   "metadata": {},
   "source": [
    "## Define Test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44184f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(model, dataloader, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "\n",
    "    for item in dataloader:\n",
    "        x = item[0].to(device).long()\n",
    "        segment_info = item[1].to(device).long()\n",
    "        label = item[2].to(device).float()\n",
    "    \n",
    "#         target_mask = (segment_info != 0)\n",
    "\n",
    "        output= model(x, segment_info)\n",
    "        loss = criterion(output, label)\n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "        \n",
    "        \n",
    "        target_mask = (segment_info != 0)  # Create a mask indicating which values in segment_info are not 0\n",
    "        \n",
    "        last_nonzero_idx = target_mask.sum(dim=1) - 1  # Find the last non-zero value for each batch\n",
    "        \n",
    "        output = output[range(last_nonzero_idx.size()[0]), last_nonzero_idx]  # Index into the output tensor using last_nonzero_idx\n",
    "        label = label[range(last_nonzero_idx.size()[0]), last_nonzero_idx]  # Index into the label tensor using last_nonzero_idx\n",
    "\n",
    "#         output = torch.masked_select(output, target_mask)\n",
    "#         label = torch.masked_select(label, target_mask)\n",
    "        pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "#         pred = (torch.sigmoid(output) >= 0.5)\n",
    "    \n",
    "        num_corrects += (pred == label).sum().item()\n",
    "        num_total += len(label)\n",
    "\n",
    "        labels.extend(label.view(-1).data.cpu().numpy())\n",
    "        outs.extend(output.view(-1).data.cpu().numpy())\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "    loss = np.mean(valid_loss)\n",
    "\n",
    "    return loss, acc, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb870691",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "824c11a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 1/10 train: - 0.4616 acc - 0.7705 auc - 0.8489\n",
      "epoch - 1/10 valid: - 0.4427 acc - 0.7850 auc - 0.8688\n",
      "epoch - 2/10 train: - 0.4437 acc - 0.7828 auc - 0.8669\n",
      "epoch - 2/10 valid: - 0.4416 acc - 0.7868 auc - 0.8707\n",
      "epoch - 3/10 train: - 0.4418 acc - 0.7835 auc - 0.8681\n",
      "epoch - 3/10 valid: - 0.4387 acc - 0.7870 auc - 0.8720\n",
      "epoch - 4/10 train: - 0.4400 acc - 0.7852 auc - 0.8695\n",
      "epoch - 4/10 valid: - 0.4383 acc - 0.7891 auc - 0.8733\n",
      "epoch - 5/10 train: - 0.4383 acc - 0.7863 auc - 0.8711\n",
      "epoch - 5/10 valid: - 0.4360 acc - 0.7898 auc - 0.8739\n",
      "epoch - 6/10 train: - 0.4367 acc - 0.7877 auc - 0.8724\n",
      "epoch - 6/10 valid: - 0.4354 acc - 0.7903 auc - 0.8751\n",
      "epoch - 7/10 train: - 0.4351 acc - 0.7890 auc - 0.8738\n",
      "epoch - 7/10 valid: - 0.4349 acc - 0.7896 auc - 0.8757\n",
      "epoch - 8/10 train: - 0.4335 acc - 0.7900 auc - 0.8751\n",
      "epoch - 8/10 valid: - 0.4351 acc - 0.7912 auc - 0.8760\n",
      "epoch - 9/10 train: - 0.4322 acc - 0.7909 auc - 0.8761\n",
      "epoch - 9/10 valid: - 0.4345 acc - 0.7911 auc - 0.8760\n",
      "epoch - 10/10 train: - 0.4314 acc - 0.7916 auc - 0.8768\n",
      "epoch - 10/10 valid: - 0.4349 acc - 0.7910 auc - 0.8760\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=MAX_LEARNING_RATE, steps_per_epoch=len(train_dataloader), epochs=EPOCHS\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "best_auc = 0\n",
    "max_steps = 3\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    loss, acc, auc = train_fn(model, train_dataloader, optimizer, scheduler, criterion, device)\n",
    "    print(\"epoch - {}/{} train: - {:.4f} acc - {:.4f} auc - {:.4f}\".format(epoch+1, EPOCHS, loss, acc, auc))\n",
    "    loss, acc, auc = valid_fn(model, valid_dataloader, criterion, device)\n",
    "    print(\"epoch - {}/{} valid: - {:.4f} acc - {:.4f} auc - {:.4f}\".format(epoch+1, EPOCHS, loss, acc, auc))\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        step = 0\n",
    "        torch.save(model.state_dict(), \"sakt_model.pt\")\n",
    "    else:\n",
    "        step += 1\n",
    "        if step >= max_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ff967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfeff59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
